{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test pyspark with AWS S3\n",
    "\n",
    "start pyspark as (Based on Spark version and Hadoop-client version, use compatible hadoop-aws library and it's dependent aws-java-sdk-bundle library and install as packages):\n",
    "\n",
    "pyspark --packages com.amazonaws:aws-java-sdk-bundle:1.11.819,org.apache.hadoop:hadoop-aws:3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version=2.4.5\n",
      "Python version=3.7.7 (v3.7.7:d7c567b08f, Mar 10 2020, 02:56:16) \n",
      "[Clang 6.0 (clang-600.0.57)]\n",
      "['/private/var/folders/8y/7n0k3c351b31f9g43spw4jqd3rym28/T/spark-89bf1f5d-6286-47b7-918b-cba4bdf05277/userFiles-8c4aaaf8-2d3e-4b35-9d5d-f7cc148704cc/org.apache.hadoop_hadoop-aws-3.2.1.jar', '/private/var/folders/8y/7n0k3c351b31f9g43spw4jqd3rym28/T/spark-89bf1f5d-6286-47b7-918b-cba4bdf05277/userFiles-8c4aaaf8-2d3e-4b35-9d5d-f7cc148704cc/com.amazonaws_aws-java-sdk-bundle-1.11.819.jar', '/private/var/folders/8y/7n0k3c351b31f9g43spw4jqd3rym28/T/spark-89bf1f5d-6286-47b7-918b-cba4bdf05277/userFiles-8c4aaaf8-2d3e-4b35-9d5d-f7cc148704cc', '/Users/chawl001/Dev/Python/Spark', '/opt/spark/spark-2.4.5-bin-without-hadoop-scala-2.12/python/lib/py4j-0.10.7-src.zip', '/opt/spark/spark-2.4.5-bin-without-hadoop-scala-2.12/python', '/Users/chawl001/Dev/Python/Spark', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python37.zip', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload', '', '/Users/chawl001/Library/Python/3.7/lib/python/site-packages', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages', '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/extensions', '/Users/chawl001/.ipython']\n"
     ]
    }
   ],
   "source": [
    "print('Spark version='+sc.version)\n",
    "import sys\n",
    "print('Python version='+ sys.version)\n",
    "print(sys.path)\n",
    "\n",
    "# imports\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, lit, count\n",
    "from pyspark.sql.functions import current_timestamp, current_date\n",
    "from pyspark.sql.functions import date_format, to_timestamp, to_date\n",
    "\n",
    "# constants\n",
    "DATETIMESTAMP_PATTERN:str = 'yyyy-MM-dd HH:mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left-side Schema:\n",
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- id: integer (nullable = false)\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- new_flag: string (nullable = true)\n",
      " |-- create_dt: string (nullable = false)\n",
      "\n",
      "+-----+---+----------------+--------+----------------+\n",
      "|name |id |dept_name       |new_flag|create_dt       |\n",
      "+-----+---+----------------+--------+----------------+\n",
      "|A    |1  |Data Science    |Y       |2020-10-16 19:26|\n",
      "|BB   |2  |null            |null    |2020-10-16 19:26|\n",
      "|CcC  |3  |Biotech         |N       |2020-10-16 19:26|\n",
      "|dddd |4  |Astrophysics    |null    |2020-10-16 19:26|\n",
      "|NaMeE|5  |Nuclear Medicine|Y       |2020-10-16 19:26|\n",
      "|666  |6  |Phycology       |N       |2020-10-16 19:26|\n",
      "+-----+---+----------------+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = [(\"A\",    1, \"Data Science\", \"Y\"),\n",
    "             (\"BB\",   2,  None, None),\n",
    "             (\"CcC\",  3, \"Biotech\", \"N\"),\n",
    "             (\"dddd\", 4, \"Astrophysics\", None),\n",
    "             (\"NaMeE\",5, \"Nuclear Medicine\", \"Y\"),\n",
    "             (\"666\",  6, \"Phycology\", \"N\")\n",
    "            ]\n",
    "\n",
    "test_schema = StructType(\n",
    "    [\n",
    "     StructField(\"name\", StringType(), nullable=False),\n",
    "     StructField(\"id\", IntegerType(), nullable=False),\n",
    "     StructField(\"dept_name\", StringType(), nullable=True),\n",
    "     StructField(\"new_flag\", StringType(), nullable=True)\n",
    "    ]\n",
    ")\n",
    "    \n",
    "test_df = spark.createDataFrame(data=test_data, schema=test_schema, verifySchema=True)\n",
    "test_df = test_df.withColumn('create_dt', date_format(current_timestamp(), DATETIMESTAMP_PATTERN))\n",
    "\n",
    "print('Left-side Schema:')\n",
    "test_df.printSchema()\n",
    "test_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to write to KMS encrypted S3 bucket...\n",
      "+----+---+------------+--------+----------------+-----------+---------------+-----------------+-------------------+-------------+\n",
      "|name|id |dept_name   |new_flag|create_dt       |SRC_KEY_VAL|SRC_CDC_OPER_NM|SRC_COMMIT_DT_UTC|TRG_CRT_DT_PART_UTC|SRC_SCHEMA_NM|\n",
      "+----+---+------------+--------+----------------+-----------+---------------+-----------------+-------------------+-------------+\n",
      "|A   |1  |Data Science|Y       |2020-10-16 19:27|0          |NONE           |2020-10-16 19:27 |2020-10-16 19:27   |N/A          |\n",
      "|BB  |2  |null        |null    |2020-10-16 19:27|0          |NONE           |2020-10-16 19:27 |2020-10-16 19:27   |N/A          |\n",
      "+----+---+------------+--------+----------------+-----------+---------------+-----------------+-------------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Successfully persisted test data file at s3a://lineardp-conformed-proposal-dev/qubole-generated-test-data/test_Schema/test_Table/\n",
      "Verifying schema of persisted dataframe:\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- new_flag: string (nullable = true)\n",
      " |-- create_dt: string (nullable = true)\n",
      " |-- SRC_KEY_VAL: string (nullable = true)\n",
      " |-- SRC_CDC_OPER_NM: string (nullable = true)\n",
      " |-- SRC_COMMIT_DT_UTC: string (nullable = true)\n",
      " |-- TRG_CRT_DT_PART_UTC: string (nullable = true)\n",
      " |-- SRC_SCHEMA_NM: string (nullable = true)\n",
      "\n",
      "Successfully verified test data file from s3a://lineardp-conformed-proposal-dev/qubole-generated-test-data/test_Schema/test_Table//*\n"
     ]
    }
   ],
   "source": [
    "# ${db.table.fullname}.*, \n",
    "# '0' AS SRC_KEY_VAL,\n",
    "# 'NONE' AS SRC_CDC_OPER_NM,\n",
    "# TO_CHAR(CURRENT_TIMESTAMP,'YYYY-MM-DD HH24:MI') AS SRC_COMMIT_DT_UTC,\n",
    "# TO_CHAR(CURRENT_TIMESTAMP,'YYYY-MM-DD HH24:MI') AS TRG_CRT_DT_PART_UTC,\n",
    "# 'N/A' AS SRC_SCHEMA_NM\n",
    "\n",
    "def persist_test_data(s3_save_location:str, df_to_persist:DataFrame, verifyPersist:bool=False) -> DataFrame :\n",
    "    kms_key_conformance = 'arn:aws:kms:us-east-1:550060283415:key/2bb9d33c-8b5b-4f67-bccc-6d9f603d7609'\n",
    "    \n",
    "    # set credential provider\n",
    "    spark._jsc.hadoopConfiguration().set('fs.s3a.aws.credentials.provider', 'com.amazonaws.auth.DefaultAWSCredentialsProviderChain')\n",
    "    \n",
    "    # s3a specific parameters using KMS\n",
    "    spark._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 's3.us-east-1.amazonaws.com')\n",
    "    spark._jsc.hadoopConfiguration().set('fs.s3a.server-side-encryption-algorithm', 'SSE-KMS')\n",
    "    spark._jsc.hadoopConfiguration().set('fs.s3a.impl.disable.cache', 'true')\n",
    "    spark._jsc.hadoopConfiguration().set('fs.s3a.server-side-encryption.key', kms_key_conformance)\n",
    "    \n",
    "    print('Ready to write to KMS encrypted S3 bucket...')\n",
    "    \n",
    "    df_with_CDC_fields = df_to_persist \\\n",
    "        .withColumn('SRC_KEY_VAL', lit('0')) \\\n",
    "        .withColumn('SRC_CDC_OPER_NM', lit('NONE')) \\\n",
    "        .withColumn('SRC_COMMIT_DT_UTC', date_format(current_timestamp(), DATETIMESTAMP_PATTERN)) \\\n",
    "        .withColumn('TRG_CRT_DT_PART_UTC', date_format(current_timestamp(), DATETIMESTAMP_PATTERN)) \\\n",
    "        .withColumn('SRC_SCHEMA_NM', lit('N/A'))\n",
    "    \n",
    "    if (verifyPersist):\n",
    "        df_with_CDC_fields.show(2, truncate=False)\n",
    "    \n",
    "    df_with_CDC_fields.repartition(1) \\\n",
    "        .write \\\n",
    "        .mode('overwrite') \\\n",
    "        .parquet(s3_save_location)\n",
    "    \n",
    "    print(f'Successfully persisted test data file at {s3_save_location}')\n",
    "    \n",
    "    if (verifyPersist):\n",
    "        s3_read_loc:str = f'{s3_save_location}/*'\n",
    "        test_read_df = spark.read.parquet(s3_read_loc)\n",
    "        print('Verifying schema of persisted dataframe:')\n",
    "        test_read_df.printSchema()\n",
    "        print(f'Successfully verified test data file from {s3_read_loc}')\n",
    "        \n",
    "\n",
    "        \n",
    "### Ensure to have function initiated and access to S3 and KMS\n",
    "s3_test_file_loc = 's3a://lineardp-conformed-proposal-dev/qubole-generated-test-data/test_Schema/test_Table/'\n",
    "persist_test_data(s3_save_location=s3_test_file_loc, df_to_persist=test_df, verifyPersist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right-side Schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n",
      "+---+--------+\n",
      "| id|location|\n",
      "+---+--------+\n",
      "|  1| Seattle|\n",
      "|  3|  Denver|\n",
      "|  5|  Austin|\n",
      "|  6|    null|\n",
      "+---+--------+\n",
      "\n",
      "Filtered RHS dataset (with non-null location):\n",
      "+---+--------+\n",
      "| id|location|\n",
      "+---+--------+\n",
      "|  1| Seattle|\n",
      "|  3|  Denver|\n",
      "|  5|  Austin|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "right_side_data = [(1, 'Seattle'), (3, 'Denver'), (5, 'Austin') , (6, None)]\n",
    "\n",
    "# short-cut way to define schema\n",
    "right_side_schema = 'id INTEGER, location STRING'\n",
    "\n",
    "right_side_df = spark.createDataFrame(data=right_side_data, schema=right_side_schema)\n",
    "\n",
    "print('Right-side Schema:')\n",
    "right_side_df.printSchema()\n",
    "right_side_df.show(5)\n",
    "\n",
    "print('Filtered RHS dataset (with non-null location):')\n",
    "filtered_rhs_df = right_side_df.filter('location is not null')\n",
    "filtered_rhs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Left Outer Join dataset =========\n",
      "\n",
      "Result ===\n",
      "   Note: Duplicate \"id\" column\n",
      "+-----+---+----------------+--------+----------------+----+--------+\n",
      "|name |id |dept_name       |new_flag|create_dt       |id  |location|\n",
      "+-----+---+----------------+--------+----------------+----+--------+\n",
      "|A    |1  |Data Science    |Y       |2020-10-16 19:27|1   |Seattle |\n",
      "|666  |6  |Phycology       |N       |2020-10-16 19:27|6   |null    |\n",
      "|CcC  |3  |Biotech         |N       |2020-10-16 19:27|3   |Denver  |\n",
      "|NaMeE|5  |Nuclear Medicine|Y       |2020-10-16 19:27|5   |Austin  |\n",
      "|dddd |4  |Astrophysics    |null    |2020-10-16 19:27|null|null    |\n",
      "|BB   |2  |null            |null    |2020-10-16 19:27|null|null    |\n",
      "+-----+---+----------------+--------+----------------+----+--------+\n",
      "\n",
      "Result ===\n",
      "   Note: Duplicate column resolved, if both sides of datasets have same name.\n",
      "         However, the column used for equality has been shifted as first column.\n",
      "+---+-----+----------------+--------+----------------+--------+\n",
      "|id |name |dept_name       |new_flag|create_dt       |location|\n",
      "+---+-----+----------------+--------+----------------+--------+\n",
      "|1  |A    |Data Science    |Y       |2020-10-16 19:27|Seattle |\n",
      "|6  |666  |Phycology       |N       |2020-10-16 19:27|null    |\n",
      "|3  |CcC  |Biotech         |N       |2020-10-16 19:27|Denver  |\n",
      "|5  |NaMeE|Nuclear Medicine|Y       |2020-10-16 19:27|Austin  |\n",
      "|4  |dddd |Astrophysics    |null    |2020-10-16 19:27|null    |\n",
      "|2  |BB   |null            |null    |2020-10-16 19:27|null    |\n",
      "+---+-----+----------------+--------+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n========= Left Outer Join dataset =========\\n')\n",
    "\n",
    "print('Result ===')\n",
    "print('   Note: Duplicate \"id\" column')\n",
    "left_outer_join_df = test_df.join(right_side_df, test_df.id == right_side_df.id, 'left_outer')\n",
    "# left_outer_join_df = test_df.join(right_side_df, test_df['id'] == right_side_df['id'], 'left_outer')\n",
    "left_outer_join_df.show(truncate=False)\n",
    "\n",
    "print('Result ===')\n",
    "print('   Note: Duplicate column resolved, if both sides of datasets have same name.')\n",
    "print('         However, the column used for equality has been shifted as first column.')\n",
    "left_outer_join_df = test_df.join(right_side_df, ['id'], 'left_outer')\n",
    "left_outer_join_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Left Anti Join dataset =========\n",
      "\n",
      "LHS dataset:\n",
      "+-----+---+----------------+--------+----------------+\n",
      "| name| id|       dept_name|new_flag|       create_dt|\n",
      "+-----+---+----------------+--------+----------------+\n",
      "|    A|  1|    Data Science|       Y|2020-10-16 19:34|\n",
      "|   BB|  2|            null|    null|2020-10-16 19:34|\n",
      "|  CcC|  3|         Biotech|       N|2020-10-16 19:34|\n",
      "| dddd|  4|    Astrophysics|    null|2020-10-16 19:34|\n",
      "|NaMeE|  5|Nuclear Medicine|       Y|2020-10-16 19:34|\n",
      "|  666|  6|       Phycology|       N|2020-10-16 19:34|\n",
      "+-----+---+----------------+--------+----------------+\n",
      "\n",
      "RHS dataset:\n",
      "+---+--------+\n",
      "| id|location|\n",
      "+---+--------+\n",
      "|  1| Seattle|\n",
      "|  3|  Denver|\n",
      "|  5|  Austin|\n",
      "+---+--------+\n",
      "\n",
      "Problem Statement: Find all LHS records with \"id\" matching with that from RHS\n",
      "\n",
      "---\n",
      "\n",
      "OPTION #1 : Use Equi Join to effectively filter:\n",
      "RESULT #1 : filtered Left dataset with matching \"id\" from Right dataset using \"Equi Join\"\n",
      "+---+-----+----------------+--------+----------------+\n",
      "| id| name|       dept_name|new_flag|       create_dt|\n",
      "+---+-----+----------------+--------+----------------+\n",
      "|  1|    A|    Data Science|       Y|2020-10-16 19:34|\n",
      "|  3|  CcC|         Biotech|       N|2020-10-16 19:34|\n",
      "|  5|NaMeE|Nuclear Medicine|       Y|2020-10-16 19:34|\n",
      "+---+-----+----------------+--------+----------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(4) Project [id#379, name#378, dept_name#380, new_flag#381, 2020-10-16 19:34 AS create_dt#386]\n",
      "+- *(4) SortMergeJoin [id#379], [id#514], Inner\n",
      "   :- *(1) Sort [id#379 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(id#379, 200)\n",
      "   :     +- Scan ExistingRDD[name#378,id#379,dept_name#380,new_flag#381]\n",
      "   +- *(3) Sort [id#514 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(id#514, 200)\n",
      "         +- *(2) Project [id#514]\n",
      "            +- *(2) Filter (isnotnull(location#515) && isnotnull(id#514))\n",
      "               +- Scan ExistingRDD[id#514,location#515]\n",
      "\n",
      "---\n",
      "\n",
      "OPTION #2 : Use Left Anti Join to effectively filter:\n",
      "Intermediate dataset (left_anti join) i.e. all LHS records which are not in RHS \n",
      "+----+---+------------+--------+----------------+\n",
      "|name|id |dept_name   |new_flag|create_dt       |\n",
      "+----+---+------------+--------+----------------+\n",
      "|666 |6  |Phycology   |N       |2020-10-16 19:34|\n",
      "|dddd|4  |Astrophysics|null    |2020-10-16 19:34|\n",
      "|BB  |2  |null        |null    |2020-10-16 19:34|\n",
      "+----+---+------------+--------+----------------+\n",
      "\n",
      "RESULT #2 : filtered Left dataset with matching \"id\" from Right dataset using \"intermediate Left_Anti Join\"\n",
      "+-----+---+----------------+--------+----------------+\n",
      "| name| id|       dept_name|new_flag|       create_dt|\n",
      "+-----+---+----------------+--------+----------------+\n",
      "|    A|  1|    Data Science|       Y|2020-10-16 19:34|\n",
      "|  CcC|  3|         Biotech|       N|2020-10-16 19:34|\n",
      "|NaMeE|  5|Nuclear Medicine|       Y|2020-10-16 19:34|\n",
      "+-----+---+----------------+--------+----------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "SortMergeJoin [id#379], [id#1003], LeftAnti\n",
      ":- *(2) Sort [id#379 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(id#379, 200)\n",
      ":     +- *(1) Project [name#378, id#379, dept_name#380, new_flag#381, 2020-10-16 19:34 AS create_dt#386]\n",
      ":        +- Scan ExistingRDD[name#378,id#379,dept_name#380,new_flag#381]\n",
      "+- SortMergeJoin [id#1003], [id#514], LeftAnti\n",
      "   :- *(4) Sort [id#1003 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(id#1003, 200)\n",
      "   :     +- *(3) Project [id#1003]\n",
      "   :        +- Scan ExistingRDD[name#1002,id#1003,dept_name#1004,new_flag#1005]\n",
      "   +- *(6) Sort [id#514 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(id#514, 200)\n",
      "         +- *(5) Project [id#514]\n",
      "            +- *(5) Filter (isnotnull(location#515) && isnotnull(id#514))\n",
      "               +- Scan ExistingRDD[id#514,location#515]\n",
      "\n",
      "---\n",
      "\n",
      "OPTION #3 : Use Spark SQL to effectively filter:\n",
      "Temp table \"lhs\" is available for SparkSQL\n",
      "Temp table \"rhs\" is available for SparkSQL\n",
      "Spark SQL : \n",
      "SELECT lhs.* \n",
      "FROM lhs \n",
      "WHERE\n",
      " EXISTS \n",
      " (SELECT 1 FROM rhs WHERE lhs.id = rhs.id)\n",
      "\n",
      "RESULT 3.1: All LHS records with \"id\" matching that of RHS:\n",
      "+-----+---+----------------+--------+----------------+\n",
      "| name| id|       dept_name|new_flag|       create_dt|\n",
      "+-----+---+----------------+--------+----------------+\n",
      "|    A|  1|    Data Science|       Y|2020-10-16 19:34|\n",
      "|  CcC|  3|         Biotech|       N|2020-10-16 19:34|\n",
      "|NaMeE|  5|Nuclear Medicine|       Y|2020-10-16 19:34|\n",
      "+-----+---+----------------+--------+----------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(4) Project [name#378, id#379, dept_name#380, new_flag#381, 2020-10-16 19:34 AS create_dt#386]\n",
      "+- SortMergeJoin [id#379], [id#514], LeftSemi\n",
      "   :- *(1) Sort [id#379 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(id#379, 200)\n",
      "   :     +- Scan ExistingRDD[name#378,id#379,dept_name#380,new_flag#381]\n",
      "   +- *(3) Sort [id#514 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(id#514, 200)\n",
      "         +- *(2) Project [id#514]\n",
      "            +- *(2) Filter isnotnull(location#515)\n",
      "               +- Scan ExistingRDD[id#514,location#515]\n",
      "\n",
      "Spark SQL : \n",
      "SELECT lhs.* \n",
      "FROM lhs \n",
      "WHERE\n",
      " NOT EXISTS \n",
      " (SELECT 1 FROM rhs WHERE lhs.id = rhs.id)\n",
      "\n",
      "RESULT 3.2: All LHS records with \"id\" NOT matching that of RHS:\n",
      "+----+---+------------+--------+----------------+\n",
      "|name| id|   dept_name|new_flag|       create_dt|\n",
      "+----+---+------------+--------+----------------+\n",
      "| 666|  6|   Phycology|       N|2020-10-16 19:34|\n",
      "|dddd|  4|Astrophysics|    null|2020-10-16 19:34|\n",
      "|  BB|  2|        null|    null|2020-10-16 19:34|\n",
      "+----+---+------------+--------+----------------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(4) Project [name#378, id#379, dept_name#380, new_flag#381, 2020-10-16 19:34 AS create_dt#386]\n",
      "+- SortMergeJoin [id#379], [id#514], LeftAnti\n",
      "   :- *(1) Sort [id#379 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(id#379, 200)\n",
      "   :     +- Scan ExistingRDD[name#378,id#379,dept_name#380,new_flag#381]\n",
      "   +- *(3) Sort [id#514 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(id#514, 200)\n",
      "         +- *(2) Project [id#514]\n",
      "            +- *(2) Filter isnotnull(location#515)\n",
      "               +- Scan ExistingRDD[id#514,location#515]\n"
     ]
    }
   ],
   "source": [
    "print('\\n========= Left Anti Join dataset =========\\n')\n",
    "\n",
    "print('LHS dataset:')\n",
    "test_df.show()\n",
    "print('RHS dataset:')\n",
    "filtered_rhs_df.show()\n",
    "print('Problem Statement: Find all LHS records with \"id\" matching with that from RHS')\n",
    "\n",
    "print('\\n---\\n')\n",
    "\n",
    "print('OPTION #1 : Use Equi Join to effectively filter:')\n",
    "print('RESULT #1 : filtered Left dataset with matching \"id\" from Right dataset using \"Equi Join\"')\n",
    "result_df_option_1 = test_df.join(filtered_rhs_df.select('id'), ['id'])\n",
    "result_df_option_1.show()\n",
    "result_df_option_1.explain()\n",
    "\n",
    "print('\\n---\\n')\n",
    "\n",
    "print('OPTION #2 : Use Left Anti Join to effectively filter:')\n",
    "left_anti_join_df = test_df.join(filtered_rhs_df, test_df['id'] == filtered_rhs_df['id'], 'left_anti')\n",
    "print('Intermediate dataset (left_anti join) i.e. all LHS records which are not in RHS ')\n",
    "left_anti_join_df.show(truncate=False)\n",
    "print('RESULT #2 : filtered Left dataset with matching \"id\" from Right dataset using \"intermediate Left_Anti Join\"')\n",
    "result_df_option_2 = test_df.join(left_anti_join_df, test_df.id == left_anti_join_df.id, 'left_anti')\n",
    "result_df_option_2.show()\n",
    "result_df_option_2.explain()\n",
    "\n",
    "print('\\n---\\n')\n",
    "\n",
    "print('OPTION #3 : Use Spark SQL to effectively filter:')\n",
    "test_df.createOrReplaceTempView('lhs')\n",
    "print('Temp table \"lhs\" is available for SparkSQL')\n",
    "filtered_rhs_df.createOrReplaceTempView('rhs')\n",
    "print('Temp table \"rhs\" is available for SparkSQL')\n",
    "\n",
    "sparkSql = '''\n",
    "SELECT lhs.* \n",
    "FROM lhs \n",
    "WHERE\n",
    " EXISTS \n",
    " (SELECT 1 FROM rhs WHERE lhs.id = rhs.id)\n",
    "'''\n",
    "print(f'Spark SQL : {sparkSql}')\n",
    "\n",
    "result_df_option_3_1 = spark.sql(sparkSql)\n",
    "print('RESULT 3.1: All LHS records with \"id\" matching that of RHS:')\n",
    "result_df_option_3_1.show()\n",
    "result_df_option_3_1.explain()\n",
    "print()\n",
    "\n",
    "sparkSql = '''\n",
    "SELECT lhs.* \n",
    "FROM lhs \n",
    "WHERE\n",
    " NOT EXISTS \n",
    " (SELECT 1 FROM rhs WHERE lhs.id = rhs.id)\n",
    "'''\n",
    "print(f'Spark SQL : {sparkSql}')\n",
    "result_df_option_3_2 = spark.sql(sparkSql)\n",
    "print('RESULT 3.2: All LHS records with \"id\" NOT matching that of RHS:')\n",
    "result_df_option_3_2.show()\n",
    "result_df_option_3_2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
